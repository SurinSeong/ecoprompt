{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f180b096-6669-4f6a-9b51-cf153ac73a66",
   "metadata": {},
   "source": [
    "# AWQ 양자화한 Midm-2.0-Mini (2.3B) vllm에서 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81667ad9-9255-40ca-ad68-3923fb01134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surin/practice-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 17:11:39 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440403e6-e06b-4484-b7bc-0db00e5bc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"./../quantization/midm-2.0-mini-awq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb2be04-b3ac-4dc0-939d-8a93c13a0475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 17:15:55 [utils.py:328] non-default args: {'max_model_len': 2048, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': './../quantization/midm-2.0-mini-awq'}\n",
      "INFO 10-16 17:15:56 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
      "INFO 10-16 17:15:56 [__init__.py:1815] Using max model len 2048\n",
      "INFO 10-16 17:15:56 [awq_marlin.py:117] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 10-16 17:15:56 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:15:56 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:15:56 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='./../quantization/midm-2.0-mini-awq', speculative_config=None, tokenizer='./../quantization/midm-2.0-mini-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=./../quantization/midm-2.0-mini-awq, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m WARNING 10-16 17:15:57 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:15:57 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m WARNING 10-16 17:15:57 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:15:57 [gpu_model_runner.py:2338] Starting to load model ./../quantization/midm-2.0-mini-awq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1016 17:15:57.149365600 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:15:58 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:15:58 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.83it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.82it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:15:59 [default_loader.py:268] Loading weights took 0.61 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:15:59 [gpu_model_runner.py:2392] Model loading took 1.4567 GiB and 1.183562 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:06 [backends.py:539] Using cache directory: /home/surin/.cache/vllm/torch_compile_cache/066a12e567/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:06 [backends.py:550] Dynamo bytecode transform time: 6.74 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:08 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:13 [backends.py:215] Compiling a graph for dynamic shape takes 4.44 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:14 [monitor.py:34] torch.compile takes 11.18 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:18 [gpu_worker.py:298] Available KV cache memory: 2.10 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:18 [kv_cache_utils.py:864] GPU KV cache size: 11,440 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:18 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 5.59x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████| 67/67 [00:07<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:27 [gpu_model_runner.py:3118] Graph capturing finished in 9 secs, took 1.08 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:27 [gpu_worker.py:391] Free memory on device (4.95/6.0 GiB) on startup. Desired GPU memory utilization is (0.8, 4.8 GiB). Actual usage is 1.46 GiB for weight, 1.23 GiB for peak activation, 0.02 GiB for non-torch memory, and 1.08 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=930561843` to fit into requested memory, or `--kv-cache-memory=1091570688` to fully utilize gpu memory. Current kv cache memory in use is 2249670451 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=89187)\u001b[0;0m INFO 10-16 17:16:27 [core.py:218] init engine (profile, create kv cache, warmup model) took 27.69 seconds\n",
      "INFO 10-16 17:16:28 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-16 17:16:28 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=BASE_MODEL,\n",
    "    max_model_len=2048,\n",
    "    gpu_memory_utilization=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ec04b4-2596-487d-a97f-b6f2c90f8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eea8aee0-722b-48c4-870c-1fa541e48532",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"파이썬 merge sort 코드와 자세한 설명도 같이 제시해줘\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"당신은 훌륭한 AI 비서입니다. 짧은 답변을 제시하고, 다음으로 상세 설명을 해주세요. You are a great AI assistant. Give a short ansewer, then elaborate. \"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": instruction\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt_message = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f660ef51-6f28-422d-90d4-2712cb4175cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 108.81it/s]\n",
      "Processed prompts: 100%|█| 1/1 [00:08<00:00,  8.32s/it, est. speed input: 66.50 toks/s, output: 61.81 t\n"
     ]
    }
   ],
   "source": [
    "eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "# Set decoding params\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=2048,\n",
    "    # repetition_penalty=1.01\n",
    ")\n",
    "\n",
    "\n",
    "outputs = llm.generate(prompt_message, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa02712f-1f2d-418e-ae5f-803d79466d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬에서 merge sort 알고리즘을 구현하는 코드와 설명은 다음과 같습니다.\n",
      "\n",
      "```python\n",
      "def merge_sort(arr):\n",
      "  # base case: 배열의 길이가 1 이하이면 이미 정렬된 상태\n",
      "  if len(arr) <= 1:\n",
      "    return arr\n",
      "\n",
      "  # 배열을 두 부분으로 나눔\n",
      "  mid = len(arr) // 2\n",
      "  left = arr[:mid]\n",
      "  right = arr[mid:]\n",
      "\n",
      "  # 재귀적으로 왼쪽과 오른쪽 부분을 정렬\n",
      "  left = merge_sort(left)\n",
      "  right = merge_sort(right)\n",
      "\n",
      "  # 정렬된 두 부분을 병합\n",
      "  return merge(left, right)\n",
      "\n",
      "def merge(left, right):\n",
      "  result = []\n",
      "  i = j = 0\n",
      "\n",
      "  # 두 배열의 요소를 비교하며 정렬\n",
      "  while i < len(left) and j < len(right):\n",
      "    if left[i] < right[j]:\n",
      "      result.append(left[i])\n",
      "      i += 1\n",
      "    else:\n",
      "      result.append(right[j])\n",
      "      j += 1\n",
      "\n",
      "  # 남은 요소 추가\n",
      "  result.extend(left[i:])\n",
      "  result.extend(right[j:])\n",
      "\n",
      "  return result\n",
      "\n",
      "# 사용 예시\n",
      "arr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\n",
      "sorted_arr = merge_sort(arr)\n",
      "print(\"정렬된 배열:\", sorted_arr)\n",
      "```\n",
      "\n",
      "**설명:**\n",
      "\n",
      "1. **merge_sort 함수:**\n",
      "   - 입력 배열의 길이가 1 이하이면 그대로 반환합니다(기본 경우).\n",
      "   - 배열을 중간 지점을 기준으로 두 부분으로 나눕니다.\n",
      "   - 각 부분을 재귀적으로 정렬합니다.\n",
      "   - 정렬된 두 부분을 `merge` 함수를 통해 병합합니다.\n",
      "\n",
      "2. **merge 함수:**\n",
      "   - 두 정렬된 배열을 비교하여 작은 값을 먼저 결과 배열에 추가합니다.\n",
      "   - 한쪽 배열이 모두 추가된 후, 남은 배열의 요소를 결과 배열에 추가합니다.\n",
      "   - 두 배열을 모두 순회한 후, 두 배열의 요소를 모두 포함한 정렬된 배열을 반환합니다.\n",
      "\n",
      "3. **사용 예시:**\n",
      "   - `merge_sort` 함수를 사용하여 입력 배열을 정렬합니다.\n",
      "   - 정렬된 배열을 출력하여 결과를 확인합니다.\n",
      "\n",
      "이 코드는 merge sort 알고리즘을 구현하며, 배열을 반복적으로 분할하고 병합하여 정렬합니다. merge sort는 O(n log n)의 시간 복잡도를 가지며, 안정적인 정렬 알고리즘으로, 대용량 데이터 정렬에 적합합니다.\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882e273-b678-4c6e-bf39-3d98f713311d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
