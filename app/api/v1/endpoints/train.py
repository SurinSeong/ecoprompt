from fastapi import APIRouter, Depends
import torch
import os

from app.core.config import base_settings
from app.models.llm_loader import get_tokenizer_2
from app.schemas.train import TrainRequest, TrainResponse
from app.services.dpo_train import train_model
from app.services.load_dpo_datasets import process_training_data
from app.services.evaluate import evaluate_model

router = APIRouter()

@router.post("", response_model=TrainResponse, status_code=200)
async def train(request: TrainRequest, tokenizer=Depends(get_tokenizer_2)):
    """ëª¨ë¸ í•™ìŠµí•˜ê¸°"""

    if request.start_training:
        # ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•œ ë°ì´í„° ë°›ê¸°
        training_dataset = request.training_data

    if training_dataset:

        print("[START] ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        # ë°ì´í„° ì²˜ë¦¬
        final_dataset = process_training_data(tokenizer, training_dataset)

        print("[COMPLETED] ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")

        try:
            print("[START] ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            train_model(final_dataset, tokenizer)
            print("[COMPLETED] ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
            try:
                # ëª¨ë¸ ì„±ëŠ¥í‰ê°€ => HAERAE Benchmark ì‚¬ìš©í•˜ê¸° + RAG ì„±ëŠ¥í‰ê°€
                print("[START] ì„±ëŠ¥ í‰ê°€")
                result = evaluate_model(base_settings.base_model + "/midm")
                print("[COMPLETED] ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ")
                
                os.rename(base_settings.base_model + "/midm", base_settings.base_model + "/midm_pre")
                print("ê¸°ì¡´ ëª¨ë¸ /midmì„ /midm_preë¡œ ë³€ê²½ ì™„ë£Œ")

                # ìƒˆë¡œìš´ ëª¨ë¸ì„ v_latestë¡œ ë³€ê²½í•˜ê¸°
                os.rename(base_settings.base_model + "/dpo_model", base_settings.base_model + "/midm")
                print("ìƒˆë¡œìš´ ëª¨ë¸ /dpo_modelì„ /midmìœ¼ë¡œ ë³€ê²½")

                return {"is_completed": result}
            
            except Exception as e:
                print(f"[ERROR] {e}")
        
        except RuntimeError as e:
            if "out of memory" in str(e):
                torch.cuda.empty_cache()
                print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ ë°°ì¹˜/ê¸¸ì´ ì¤„ì´ê¸°")
                return {"is_completed": False}
            
            elif "device-side assert triggered" in str(e):
                print("âš ï¸ CUDA Assert ë°œìƒ â†’ ë°ì´í„°ì…‹ ì¸ë±ìŠ¤ë‚˜ ë¼ë²¨ í™•ì¸í•˜ê¸°")
                return {"is_completed": False}

            elif "Tokenizer" in str(e):
                print("âš ï¸ Tokenizer ë¬¸ì œ â†’ pad_token ì„¤ì • í™•ì¸.")
                return {"is_completed": False}

            else:
                print(f"ğŸš¨ Unknown RuntimeError: {e}")
                return {"is_completed": False}
            
        except ValueError as e:
            print(f"âš ï¸ ValueError: {e}")
            return {"is_completed": False}
        
        except Exception as e:
            print(f"ğŸš¨ Unexpected error: {type(e).__name__}: {e}")
            return {"is_completed": False}
    
        except Exception as e:
            print(f"ğŸš¨ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ ì—†ìŒ: {type(e).__name__}: {e}")
            return {"is_completed": False}