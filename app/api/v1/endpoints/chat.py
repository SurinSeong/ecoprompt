import json
from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse

from app.schemas.chat import ChatRequest, ChatResponse
from app.services.chat import stream_response
from app.models.llm_loader import get_llm, get_tokenizer
from app.models.vectordb_loader import get_vector_store
from app.models.mongodb_loader import find_chatting_id, get_chat_history

router = APIRouter()

@router.post("")
async def chat(request: ChatRequest, llm=Depends(get_llm), tokenizer=Depends(get_tokenizer), vector_store=Depends(get_vector_store)):
    """
    ìŠ¤íŠ¸ë¦¼ ë‹µë³€ ì œê³µ
    """
    user_input = request.user_input
    personal_prompt = request.personal_prompt
    message_uuid = str(request.message_uuid)

    # ì‚¬ìš©ì ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
    chatting_id = find_chatting_id(message_uuid)
    if chatting_id:
        chat_history = get_chat_history(int(chatting_id))
        # contentì™€ senderTypeì„ ì¡°í•©í•´ì„œ histroy ìƒì„±í•˜ëŠ” ì½”ë“œ í•„ìš”í•¨.
    else:
        chat_history = None

    chain = stream_response(vector_store=vector_store, llm=llm, tokenizer=tokenizer)
    payload = {
        "personal_prompt": personal_prompt,
        "question": user_input,
        "history": "",
        "context": ""    # ì¶”í›„ chat_historyë¡œ ë³€ê²½ì˜ˆì •
    }

    async def event_generator():
        state = "SEEK_OPEN_CHOSEN"
        chosen_response = ""
        rejected_response = ""

        OPEN_C  = "<CHOSEN>"
        CLOSE_C = "</CHOSEN>"
        OPEN_R  = "<REJECTED>"
        CLOSE_R = "</REJECTED>"
        
        sequence_id = -1
        try:
            async for chunk in chain.astream(payload):
                # print(chunk)
                if not chunk:
                    continue

                # # ğŸ”¥ ì—¬ê¸°ê°€ í•µì‹¬ â€” chunkë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                # print(chunk)
                # if not isinstance(chunk, str):
                #     try:
                #         # AIMessageChunk ê°™ì€ ê²½ìš° .content ì†ì„±
                #         if hasattr(chunk, "content"):
                #             chunk = chunk.content or ""
                #         # dictë©´ content / delta / text ì¤‘ ì¶”ì¶œ
                #         elif isinstance(chunk, dict):
                #             chunk = (
                #                 chunk.get("content")
                #                 or chunk.get("delta", {}).get("content")
                #                 or chunk.get("text")
                #                 or json.dumps(chunk, ensure_ascii=False)
                #             )
                #         else:
                #             chunk = str(chunk)
                #     except Exception as e:
                #         print(f"[WARN] Failed to normalize chunk: {type(chunk)} - {e}")
                #         continue
                
                sequence_id += 1

                if (state == "SEEK_OPEN_CHOSEN") and (OPEN_C in chunk):
                    state = "FOUND_CHOSEN"
                    yield f"data: {ChatResponse(sequence_id=sequence_id, token="START").model_dump_json()}\n\n"
                    continue
                

                if (state == "FOUND_CHOSEN") and (OPEN_C not in chunk) and (CLOSE_C not in chunk):
                    chosen_response += chunk
                    yield f"data: {ChatResponse(sequence_id=sequence_id, token=chunk).model_dump_json()}\n\n"
                    continue

                
                if (state == "FOUND_CHOSEN") and (OPEN_C not in chunk) and (CLOSE_C in chunk):
                    state = "END_CHOSEN"
                    yield f"data: {ChatResponse(sequence_id=sequence_id, token='DONE').model_dump_json()}\n\n"
                    continue


                if (state == "END_CHOSEN") and (OPEN_R in chunk):
                    state = "FOUND_REJECTED"
                    continue


                if (state == "FOUND_REJECTED") and (OPEN_R not in chunk) and (CLOSE_R not in chunk):
                    rejected_response += chunk
                    continue


                if (state == "FOUND_REJECTED") and (OPEN_R not in chunk) and (CLOSE_R in chunk):
                    break

            print(f"[CHOSEN]\n{chosen_response}")
        
        except Exception as e:
            # print(chunk)
            # ì˜¤ë¥˜ ì‹œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
            yield f"data: [ERROR] {type(e).__name__}: {e}\n\n"

        print(f"[REJECTED]\n{rejected_response}")

        # rejected response ì €ì¥í•˜ê¸°

    return StreamingResponse(event_generator(), media_type="text/event-stream")


# @router.post("")
# async def stream(request: ChatRequest, llm=Depends(get_llm_engine), tokenizer=Depends(get_tokenizer)):
#     """vLLM ìŠ¤íŠ¸ë¦¼ì„ ë°›ì•„ SSE í˜•ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì‘ë‹µ"""
#     sampling_params = load_sampling_params()
#     request_id = request.request_id
#     user_input = request.user_input

#     # ìµœì¢… ê²°ê³¼ë¥¼ ë‹´ì„ ì»¨í…Œì´ë„ˆ ìƒì„±
#     final_responses = {"chosen": "", "rejected": ""}

#     # ì„œë¹„ìŠ¤ í•¨ìˆ˜ í˜¸ì¶œ
#     stream_generator = generate_sse_stream(
#         llm,
#         request_id,
#         user_input,
#         sampling_params,
#         final_responses
#     )

#     # DBì— rejected ì €ì¥í•˜ê¸°

#     return StreamingResponse(stream_generator, media_type="text/event-stream")
