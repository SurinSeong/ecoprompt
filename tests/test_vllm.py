import os

os.environ["VLLM_USE_V1"] = "1"

import asyncio
from vllm import SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.sampling_params import RequestOutputKind
from vllm.v1.engine.async_llm import AsyncLLM
from transformers import AutoTokenizer

MODEL_PATH_1 = "./local-models/Qwen2.5-Coder-0.5B-Instruct"    # ì›ë³¸ ëª¨ë¸ ê²½ë¡œ
MODEL_PATH_2 = "./local-models/Llama-3.2-1B-Instruct"       # llama ì†Œí˜• ëª¨ë¸

# MODEL_PATH = "./local-models/Midm-2.0-Mini-Instruct"

tokenizer_1 = AutoTokenizer.from_pretrained(
    pretrained_model_name_or_path=MODEL_PATH_1,
    trust_remote_code=True
)

tokenizer_2 = AutoTokenizer.from_pretrained(
    pretrained_model_name_or_path=MODEL_PATH_2,
)
tokenizer_2.pad_token = tokenizer_2.eos_token
tokenizer_2.padding_side = "left"

chosen_prompt = """ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™” ê¸°ë¡ê³¼ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ, ì‚¬ìš©ìê°€ ì„ í˜¸í• ë§Œí•œ ë‹µë³€ì„ ë°˜í™˜í•˜ì„¸ìš”.
ì•„ë˜ ì¶œë ¥ í˜•ì‹ì„ 100% ì¤€ìˆ˜í•´ì£¼ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
<ANSWER>
ë‹µë³€
</ANSWER>

---
[ê·œì¹™]:
1) ë¬´ì¡°ê±´ ë‹µë³€ì€ íƒœê·¸ ì•ˆì—, ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. 
2) ì„ í˜¸í• ë§Œí•œ ë‹µë³€ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì•Œë§ì€ ë‹µë³€ì…ë‹ˆë‹¤. ìì„¸í•˜ê³  ì •í™•í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ì½”ë“œë¥¼ ì•Œë ¤ë‹¬ë¼ëŠ” ì§ˆë¬¸ì—ëŠ” ì½”ë“œë¸”ë¡ì„ ì‚¬ìš©í•´ì„œ ì½”ë“œë¥¼ ì¶œë ¥í•˜ê³  ê·¸ì— ëŒ€í•œ ì„¤ëª…ë„ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”.
3) ë°˜ë“œì‹œ ì¶œë ¥í˜•ì‹ì„ ì§€ì¼œì„œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
4) Contextê°€ ìˆìœ¼ë©´ ìš°ì„  í™œìš©í•˜ê³ , ì—†ìœ¼ë©´ ì•„ëŠ” ë‚´ìš©ì€ ë°˜í™˜í•˜ê³ , ëª¨í˜¸í•œ ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•œ ë’¤ ì§ˆë¬¸ ì¬ìš”ì²­ ë˜ëŠ” ê²€ìƒ‰ì„ ì œì•ˆí•˜ì„¸ìš”.
5) ì´ì „ ëŒ€í™”ì™€ ë¬¸ë§¥ì´ ì´ì–´ì§€ë©´ Historyë¥¼ ë°˜ì˜í•´ì£¼ì„¸ìš”.
6) ë¯¼ê°/ìœ„í—˜ ì£¼ì œëŠ” ì•ˆì „ ê°€ì´ë“œë¥¼ ì¤€ìˆ˜í•˜ì„¸ìš”.
7) </ANSWER> ì´í›„ì—ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
8) ì‚¬ìš©ì ì§€ì¹¨ì´ ìˆë‹¤ë©´ ê¼­ ì°¸ê³ í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
9) ê¸°ë³¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

---
"""

rejected_prompt = """
ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™” ê¸°ë¡ê³¼ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ, ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ì§€ ì•Šì„ë§Œí•œ ë‹µë³€ì„ ë°˜í™˜í•˜ì„¸ìš”.
ì•„ë˜ ì¶œë ¥ í˜•ì‹ì„ 100% ì¤€ìˆ˜í•´ì£¼ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
<ANSWER>
ë‹µë³€
</ANSWER>

---
[ê·œì¹™]:
1) ë¬´ì¡°ê±´ ë‹µë³€ì€ íƒœê·¸ ì•ˆì—, ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. 
2) ì„ í˜¸í•˜ì§€ ì•Šì„ë§Œí•œ ë‹µë³€ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì•Œë§ì§€ ì•Šì€ ë‹µë³€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ì•Œë§ì§€ ì•Šì€ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
3) ë°˜ë“œì‹œ ì¶œë ¥í˜•ì‹ì„ ì§€ì¼œì„œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
4) ì´ì „ ëŒ€í™”ì™€ ë¬¸ë§¥ì´ ì´ì–´ì§€ë©´ Historyë¥¼ ë°˜ì˜í•´ì£¼ì„¸ìš”.
5) ë¯¼ê°/ìœ„í—˜ ì£¼ì œëŠ” ì•ˆì „ ê°€ì´ë“œë¥¼ ì¤€ìˆ˜í•˜ì„¸ìš”.
6) </ANSWER> ì´í›„ì—ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
7) ì‚¬ìš©ì ì§€ì¹¨ì´ ìˆë‹¤ë©´ ê¼­ ì°¸ê³ í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
8) ê¸°ë³¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

---
"""

async def stream_response(engine_1: AsyncLLM, engine_2: AsyncLLM, prompt: str, request_id: str) -> None:
    print(f"\nğŸš€ Prompt: {prompt}")
    print("ğŸ’¬ Response: ", end="", flush=True)

    # smapling params ì„¤ì •
    sampling_params_1 = SamplingParams(
        max_tokens=2048,
        temperature=0.3,
        top_p=0.95,
        seed=42,
        output_kind=RequestOutputKind.DELTA,
        repetition_penalty=1.01,
        frequency_penalty=0.2,
        presence_penalty=0.1
    )

    sampling_params_2 = SamplingParams(
        max_tokens=2048,
        temperature=0.9,
        top_p=0.95,
        seed=42,
        repetition_penalty=1.01,
        frequency_penalty=0.2,
        presence_penalty=0.1
    )

    chosen_messages = [
        {"role": "system", "content": chosen_prompt},
        {"role": "user", "content": prompt}
    ]

    prompt_1 = tokenizer_1.apply_chat_template(
        chosen_messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True
    )

    rejected_messages = [
        {"role": "system", "content": rejected_prompt},
        {"role": "user", "content": prompt}
    ]

    prompt_2 = tokenizer_2.apply_chat_template(
        rejected_messages,
        tokenize=False,
        add_generation_prompt=True,
    )

    try:
        async for output in engine_1.generate(
            prompt=prompt_1, sampling_params=sampling_params_1, request_id=request_id
        ):
            for completion in output.outputs:
                new_text = completion.text
                if new_text:
                    print(new_text, end="", flush=True)

            if output.finished:
                print("\nâœ… Generation complete!")
                break

        async for output in engine_2.generate(
            prompt=prompt_2, sampling_params=sampling_params_2, request_id=request_id
        ):  
            total_answer = ""

            if output.finished:
                total_answer = output.outputs[0].text
                print(total_answer, flush=True)
                print("\nâœ… Generation complete!")
                break


    except Exception as e:
        print(f"\nâŒ Error during streaming: {e}")
        raise


async def main():
    print("ğŸ”§ Initializing AsyncLLM...")
    engine_args_1 = AsyncEngineArgs(
        model=MODEL_PATH_1,
        enforce_eager=True,
        gpu_memory_utilization=0.45,
        trust_remote_code=True,
        quantization="fp8",
        max_model_len=8192

    )
    engine_1 = AsyncLLM.from_engine_args(engine_args_1)

    engine_args_2 = AsyncEngineArgs(
        model=MODEL_PATH_2,
        enforce_eager=True,
        gpu_memory_utilization=0.65,
        quantization="bitsandbytes",
        max_model_len=8192
    )
    engine_2 = AsyncLLM.from_engine_args(engine_args_2)

    try:
        prompt = "íŒŒì´ì¬ merge sortì— ëŒ€í•´ ì½”ë“œ ì‘ì„±í•˜ê³  ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì¤˜."
        print("ğŸ¯ Running streaming examples...")

        request_id = "stream-example-1"

        await stream_response(engine_1, engine_2, prompt, request_id)

        # if i < len(prompts):
        #     await asyncio.sleep(0.5)

        print("\nğŸ‰ All streaming examples completed!")
    
    finally:
        print("ğŸ”§ Shutting down engine...")
        engine_1.shutdown()
        engine_2.shutdown()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ Interrupted by user")