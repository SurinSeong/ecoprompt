import os

os.environ["VLLM_USE_V1"] = "1"

import asyncio
from vllm import SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.sampling_params import RequestOutputKind
from vllm.v1.engine.async_llm import AsyncLLM
from transformers import AutoTokenizer

MODEL_PATH = "./local-models/Qwen2.5-Coder-0.5B-Instruct"    # ì›ë³¸ ëª¨ë¸ ê²½ë¡œ

# MODEL_PATH = "./local-models/Midm-2.0-Mini-Instruct"

tokenizer = AutoTokenizer.from_pretrained(
    pretrained_model_name_or_path=MODEL_PATH,
    trust_remote_code=True
)
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "right"

rag_prompt = """ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™” ê¸°ë¡ê³¼ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ, ë‹µë³€ì„ ë°˜í™˜í•˜ì„¸ìš”.
ì•„ë˜ ì¶œë ¥ í˜•ì‹ì„ 100% ì¤€ìˆ˜í•´ì£¼ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
<CHOSEN>
ì„ í˜¸ ë‹µë³€
</CHOSEN>

<REJECTED>
ë¹„ì„ í˜¸ ë‹µë³€
</REJECTED>

---
[ê·œì¹™]:
1) ë¬´ì¡°ê±´ ëª¨ë“  ë‹µë³€ì€ ê° íƒœê·¸ ì•ˆì— ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. 
2) ì„ í˜¸ ë‹µë³€ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì•Œë§ì€ ë‹µë³€ì…ë‹ˆë‹¤. ìì„¸í•˜ê³  ì •í™•í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ì½”ë“œë¥¼ ì•Œë ¤ë‹¬ë¼ëŠ” ì§ˆë¬¸ì—ëŠ” ì½”ë“œë¸”ë¡ì„ ì‚¬ìš©í•´ì„œ ì½”ë“œë¥¼ ì¶œë ¥í•˜ê³  ê·¸ì— ëŒ€í•œ ì„¤ëª…ë„ í•¨ê»˜ <CHOSEN> íƒœê·¸ ì•ˆì— ì œê³µí•´ì£¼ì„¸ìš”.
3) ë¹„ì„ í˜¸ ë‹µë³€ì€ ì‚¬ìš©ìê°€ ê¶ê¸ˆì¦ì„ í•´ê²°í•˜ê¸°ì—ëŠ” ë¶€ì¡±í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ë‹µë³€ì…ë‹ˆë‹¤. ì•Œë§ì§€ ì•Šì€ ë‹µë³€ì„ <REJECTED> íƒœê·¸ ì•ˆì— ì‘ì„±í•˜ì„¸ìš”.
4) ë°˜ë“œì‹œ ì¶œë ¥í˜•ì‹ì„ ì§€ì¼œì„œ, ì„ í˜¸ ë‹µë³€, ë¹„ì„ í˜¸ ë‹µë³€ ëª¨ë‘ ìƒì„±í•´ì£¼ì„¸ìš”.
5) Contextê°€ ìˆìœ¼ë©´ ìš°ì„  í™œìš©í•˜ê³ , ì—†ìœ¼ë©´ ì•„ëŠ” ë‚´ìš©ì€ ë°˜í™˜í•˜ê³ , ëª¨í˜¸í•œ ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•œ ë’¤ ì§ˆë¬¸ ì¬ìš”ì²­ ë˜ëŠ” ê²€ìƒ‰ì„ ì œì•ˆí•˜ì„¸ìš”.
6) ì´ì „ ëŒ€í™”ì™€ ë¬¸ë§¥ì´ ì´ì–´ì§€ë©´ Historyë¥¼ ë°˜ì˜í•´ì£¼ì„¸ìš”.
7) ë¯¼ê°/ìœ„í—˜ ì£¼ì œëŠ” ì•ˆì „ ê°€ì´ë“œë¥¼ ì¤€ìˆ˜í•˜ì„¸ìš”.
8) </REJECTED> ì´í›„ì—ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
9) ì‚¬ìš©ì ì§€ì¹¨ì´ ìˆë‹¤ë©´ ê¼­ ì°¸ê³ í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
10) ê¸°ë³¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

---
"""

async def stream_response(engine: AsyncLLM, prompt: str, request_id: str) -> None:
    print(f"\nğŸš€ Prompt: {prompt!r}")
    print("ğŸ’¬ Response: ", end="", flush=True)

    # smapling params ì„¤ì •
    sampling_params = SamplingParams(
        max_tokens=2048,
        temperature=0.3,
        top_p=0.95,
        seed=42,
        output_kind=RequestOutputKind.DELTA,
        repetition_penalty=1.01,
        frequency_penalty=0.2,
        presence_penalty=0.1
    )

    messages = [
        {"role": "system", "content": rag_prompt},
        {"role": "user", "content": prompt}
    ]

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True
    )

    try:
        async for output in engine.generate(
            prompt=prompt, sampling_params=sampling_params, request_id=request_id
        ):
            for completion in output.outputs:
                new_text = completion.text
                if new_text:
                    print(new_text, end="", flush=True)

            if output.finished:
                print("\nâœ… Generation complete!")
                break

    except Exception as e:
        print(f"\nâŒ Error during streaming: {e}")
        raise


async def main():
    print("ğŸ”§ Initializing AsyncLLM...")
    engine_args = AsyncEngineArgs(
        model=MODEL_PATH,
        enforce_eager=True,
        gpu_memory_utilization=0.75,
        trust_remote_code=True,
        # quantization="bitsandbytes",
        max_model_len=8192

    )
    engine = AsyncLLM.from_engine_args(engine_args)

    try:
        prompts = [
            "íŒŒì´ì¬ merge sortì— ëŒ€í•´ ì½”ë“œ ì‘ì„±í•˜ê³  ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì¤˜.",
        ]
        print(f"ğŸ¯ Running {len(prompts)} streaming examples...")

        for i, prompt in enumerate(prompts, 1):
            request_id = f"stream-example-{i}"
            await stream_response(engine, prompt, request_id)

            if i < len(prompts):
                await asyncio.sleep(0.5)

        print("\nğŸ‰ All streaming examples completed!")
    
    finally:
        print("ğŸ”§ Shutting down engine...")
        engine.shutdown()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ Interrupted by user")