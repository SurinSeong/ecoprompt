from contextlib import asynccontextmanager
from fastapi import FastAPI

from app.api.v1.routers import api_router
from app.models.llm_loader import load_tokenizer, llm_tokenizer, load_llm_engine, llm_engine
from app.models.vectordb_loader import load_vectordb, vector_store, load_embedding_model, embedding_model
from app.models.mongodb_loader import load_mongodb, mongo_client

# lifespan ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì •ì˜
@asynccontextmanager
async def lifespan_manager(app: FastAPI):

    # ğŸš€ ì„œë²„ ì‹œì‘ (Startup) ë¡œì§
    load_tokenizer()    # Tokenizer
    load_embedding_model()
    load_vectordb()
    await load_llm_engine()
    # await load_llm()    # LLM ëª¨ë¸ ë¡œë“œ (GPU ë©”ëª¨ë¦¬ ìƒì£¼ ì‹œì‘)
    await load_mongodb()    # MongoDB ë¡œë“œ
    # await load_llm_engine()
    print("Application startup complete!")

    # yieldê°€ ì‹¤í–‰ë˜ë©´ ì„œë²„ê°€ ìš”ì²­ì„ ë°›ê¸° ì‹œì‘í•¨.
    yield

    # ğŸ›‘ ì„œë²„ ì¢…ë£Œ (Shutdown) ë¡œì§
    # if llm is not None:
    #     pass

    if vector_store is not None:
        pass

    if llm_tokenizer is not None:
        pass

    if embedding_model is not None:
        pass

    if mongo_client is not None:
        pass

    if llm_engine is not None:
        pass


# cuda í™•ì¸
import torch

device = "auto" if torch.cuda.is_available() else "cpu"
print(f"Device set to use {device}")

# 1. FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI(
    title="Ecoprompt Main LLM",
    description="API Documentation",
    version="1.0.0",
    lifespan=lifespan_manager
)

# 2. ê²½ë¡œ ì‘ë™ í•¨ìˆ˜ (Route Operation) ì •ì˜
@app.get("/")
def read_root():
    """
    HTTP GET ìš”ì²­ì´ ë£¨íŠ¸ ê²½ë¡œ('/')ë¡œ ë“¤ì–´ì™”ì„ ë•Œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜
    """
    return {"message": "Hello, FastAPI"}

@app.get("/health")
def health():
    return {"status": "ok"}

# ë¼ìš°í„° ì—°ê²°
app.include_router(api_router, prefix="/api/v1/ai")