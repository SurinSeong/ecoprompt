import os

os.environ["CUDA_VISIBLE_DEVICES"] = "1"

from contextlib import asynccontextmanager
from fastapi import FastAPI

from app.api.v1.routers import api_router
from app.models.llm_loader import load_tokenizers, llm_tokenizer_1, llm_tokenizer_2, load_llm_engines, llm_engine_1, llm_engine_2
# from app.models.vectordb_loader import load_vectordb, vector_store, load_embedding_model, embedding_model
from app.models.mongodb_loader import load_mongodb, mongo_client
from app.core.concurrency import get_limiter
from app.core.config import base_settings

# lifespan ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì •ì˜
@asynccontextmanager
async def lifespan_manager(app: FastAPI):

    # ğŸš€ ì„œë²„ ì‹œì‘ (Startup) ë¡œì§
    print("="*60)
    print("ğŸš€ ì„œë²„ ì‹œì‘ ì¤‘...")
    print("="*60)

    # ë™ì‹œì„± ì œì–´ ì´ˆê¸°í™”
    limiter = get_limiter()
    print(f"âœ… ë™ì‹œì„± ì œì–´: ìµœëŒ€ {base_settings.max_concurrent_requests}ê°œ ë™ì‹œ ìš”ì²­")
    print(f"âœ… ìš”ì²­ íƒ€ì„ì•„ì›ƒ: {base_settings.request_timeout}ì´ˆ")

    load_tokenizers()    # Tokenizer
    # load_embedding_model()
    # load_vectordb()
    await load_llm_engines()
    await load_mongodb()    # MongoDB ë¡œë“œ

    print("="*60)
    print("âœ… Application startup complete!")
    print("="*60)

    # yieldê°€ ì‹¤í–‰ë˜ë©´ ì„œë²„ê°€ ìš”ì²­ì„ ë°›ê¸° ì‹œì‘í•¨.
    yield

    # ğŸ›‘ ì„œë²„ ì¢…ë£Œ (Shutdown) ë¡œì§
    print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")

    if limiter is not None:
        stats = limiter.get_stats()
        print(f"ğŸ“Š ìµœì¢… í†µê³„:")
        print(f"   - ì´ ì²˜ë¦¬ ìš”ì²­: {stats['total']}")
        print(f"   - í˜„ì¬ í™œì„± ìš”ì²­: {stats['active']}")

    # if vector_store is not None:
    #     pass

    if llm_tokenizer_1 is not None:
        pass

    if llm_tokenizer_2 is not None:
        pass

    # if embedding_model is not None:
    #     pass

    if mongo_client is not None:
        pass

    if llm_engine_1 is not None:
        pass

    if llm_engine_2 is not None:
        pass


# cuda í™•ì¸
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device set to use {device}")

# 1. FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI(
    title="Ecoprompt Main LLM",
    description="API Documentation",
    version="1.0.0",
    lifespan=lifespan_manager
)

# 2. ê²½ë¡œ ì‘ë™ í•¨ìˆ˜ (Route Operation) ì •ì˜
@app.get("/")
def read_root():
    """
    HTTP GET ìš”ì²­ì´ ë£¨íŠ¸ ê²½ë¡œ('/')ë¡œ ë“¤ì–´ì™”ì„ ë•Œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜
    """
    return {"message": "Hello, FastAPI"}

@app.get("/stats")
def get_stats():
    """ë™ì‹œì„± í†µê³„ í™•ì¸"""
    try:
        limiter = get_limiter()
        return limiter.get_stats()
    
    except Exception as e:
        return {"error": str(e)}


@app.get("/health")
def health():
    """í—¬ìŠ¤ ì²´í¬ + í†µê³„"""
    try:
        limiter = get_limiter()
        stats = limiter.get_stats()
        return {
            "status": "ok",
            "concurrency": stats,
            "config": {
                "max_concurrent_requests": base_settings.max_concurrent_requests,
                "request_timeout": base_settings.request_timeout
            }
        }
    
    except Exception as e:
        return {
            "status": "unhealthy",
            "concurrency": {"error": str(e)}
        }

# ë¼ìš°í„° ì—°ê²°
app.include_router(api_router, prefix="/api/v1/ai")